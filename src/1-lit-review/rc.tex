A new method in for real-time global illumination called emerged few years ago\footnote{This is intentionally vague, as the paper written was not dated nor published a journal. It is available freely on GitHub.} in a paper titled ``Radiance Cascades: A Novel Approach to Calculating Global Illumination'' authored by Alexander Sannikov~\cite{sannikovRadianceCascadesNovela}.
Sannikov, a video game developer, proposed a new data structure, which he coined ``radiance cascades,'' that more-effectively calculates and stores radiance field information by decomposing the domain into near- and far-field.
This idea is based on a simple concept: to resolve the radiance emitted by an object, one needs to have higher spatial resolution near the object but higher angular resolution far from the object.

Another integral concept of the technique is the idea of ray construction.
That is, instead of explicitly casting rays, each probe (within each cascade) is assigned a spherical ``shell'' region and angular discretization, which is the only part of the domain that contributes to the radiance of the probe.
The ``thickness'' of this shell is termed the ``radiance interval'' and is one of the foundational principles.
Thus, once cascade has evaluated the contributions to each probe within its respective radiance interval, these intervals can be combined into rays when needed via interpolation.

Sannikov boldly claims that this method leads to, for all practical purposes, infinite ray casting in a finite amount of time.
Indeed, the paper shows that the total memory required to store an infinite number of cascades is less than constant, that constant being exactly equal to twice the amount of memory required to store the first cascade.
This holds for \textit{both} 2D and 3D problems.
As seen with neutronics methods such as the \acrlong{moc} (and its relative, \acrlong{trrm}), the biggest limitation for resolving solutions in complex geometry is the number of rays that are able to be cast.
Though~\cite{sannikovRadianceCascadesNovela} gives a strong start to tackling this problem, Sannikov notes that the task of simply storing the first cascade is practicaly equivalent to discretizing the entire domain, which can be a dealbreaker for large problems.
That is, to truly see the benefits of this technique, computational tools must be able to effectively store \textit{twice} the memory of the finest discretization, though each additional cascade's memory requirements reduce exponentially.

